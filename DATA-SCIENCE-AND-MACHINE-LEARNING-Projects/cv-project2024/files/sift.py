# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jzq_ocqMXJMDjcylzdoc-QccYyEhSUma
"""

pip install wildlife-datasets

pip install wildlife-tools

pip install timm

"""**Importing Wildlife data set and Wildlife tools**


"""

import timm
import numpy as np
from wildlife_datasets.datasets import MacaqueFaces
from wildlife_tools.data import WildlifeDataset
import torchvision.transforms as T
from wildlife_datasets import datasets, splits
from wildlife_tools.features import DeepFeatures
from wildlife_tools.similarity import CosineSimilarity
from wildlife_tools.inference import KnnClassifier
from sklearn.metrics import precision_score, recall_score, f1_score

"""**Same Data set use for Fine tuning**

**MacaqueFaces Dataset**
"""

# Download dataset (if not already downloaded)
datasets.MacaqueFaces.get_data('../data/MacaqueFaces')

# Load dataset metadata
metadata_MacaqueFaces = datasets.MacaqueFaces('../data/MacaqueFaces')
transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])
dataset_MacaqueFaces = WildlifeDataset(metadata_MacaqueFaces.df, metadata_MacaqueFaces.root, transform=transform)

dataset_database_MacaqueFaces = WildlifeDataset(metadata_MacaqueFaces.df.iloc[100:,:], metadata_MacaqueFaces.root, transform=transform)
dataset_query_MacaqueFaces = WildlifeDataset(metadata_MacaqueFaces.df.iloc[:100,:], metadata_MacaqueFaces.root, transform=transform)

"""**Importing the pretrain Model**"""

name = 'hf-hub:BVRA/MegaDescriptor-T-224'
extractor_MacaqueFaces = DeepFeatures(timm.create_model(name, num_classes=0, pretrained=True))
query_MacaqueFaces, database_MacaqueFaces = extractor_MacaqueFaces(dataset_query_MacaqueFaces), extractor_MacaqueFaces(dataset_database_MacaqueFaces)

similarity_function = CosineSimilarity()
similarity_MacaqueFaces = similarity_function(query_MacaqueFaces, database_MacaqueFaces)
print(similarity_MacaqueFaces)

classifier_MacaqueFaces = KnnClassifier(k=1, database_labels=dataset_database_MacaqueFaces.labels_string)
predictions_MacaqueFaces = classifier_MacaqueFaces(similarity_MacaqueFaces['cosine'])
print("Predictions for 100 test Images:-\n",predictions_MacaqueFaces)
accuracy_MacaqueFaces = np.mean(dataset_query_MacaqueFaces.labels_string == predictions_MacaqueFaces)
print("Accuracy on MacaqueFaces data: {:.2f}%".format(accuracy_MacaqueFaces * 100))
# precision
precision_MacaqueFaces = precision_score(dataset_query_MacaqueFaces.labels_string, predictions_MacaqueFaces, average='weighted')
# recall
recall_MacaqueFaces = recall_score(dataset_query_MacaqueFaces.labels_string, predictions_MacaqueFaces, average='weighted')
# F1 score
f1_MacaqueFaces = f1_score(dataset_query_MacaqueFaces.labels_string, predictions_MacaqueFaces, average='weighted')
print("Precision:", precision_MacaqueFaces)
print("Recall:", recall_MacaqueFaces)
print("F1 Score:", f1_MacaqueFaces)

"""**LionData Dateset**"""

# Download dataset (if not already downloaded)
datasets.LionData.get_data('../data/LionData')

# Load dataset metadata
metadata_LionData = datasets.LionData('../data/LionData')
transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])
dataset_LionData = WildlifeDataset(metadata_LionData.df, metadata_LionData.root, transform=transform)

dataset_database_LionData = WildlifeDataset(metadata_LionData.df.iloc[100:,:], metadata_LionData.root, transform=transform)
dataset_query_LionData = WildlifeDataset(metadata_LionData.df.iloc[:100,:], metadata_LionData.root, transform=transform)

name = 'hf-hub:BVRA/MegaDescriptor-T-224'
extractor_LionData = DeepFeatures(timm.create_model(name, num_classes=0, pretrained=True))
query_LionData, database_LionData = extractor_LionData(dataset_query_LionData), extractor_LionData(dataset_database_LionData)

similarity_function = CosineSimilarity()
similarity_LionData = similarity_function(query_LionData, database_LionData)
print(similarity_LionData)

# Predictions
classifier_LionData = KnnClassifier(k=1, database_labels=dataset_database_LionData.labels_string)
predictions_LionData = classifier_LionData(similarity_LionData['cosine'])
print("Predictions for 100 test Images:-\n",predictions_LionData)
# Accuracy
accuracy_LionData = np.mean(dataset_query_LionData.labels_string == predictions_LionData)
print("Accuracy on Lion data: {:.2f}%".format(accuracy_LionData * 100))
# F1 score,recall and precision
precision = precision_score(dataset_query_LionData.labels_string, predictions_LionData, average='weighted')
recall = recall_score(dataset_query_LionData.labels_string, predictions_LionData, average='weighted')
f1 = f1_score(dataset_query_LionData.labels_string, predictions_LionData, average='weighted')
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

"""**NyalaDataSet**"""

# Download dataset (if not already downloaded)
datasets.NyalaData.get_data('../data/NyalaData')

# Load dataset metadata
metadata_N = datasets.NyalaData('../data/NyalaData')
transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])
dataset_N = WildlifeDataset(metadata_N.df, metadata_N.root, transform=transform)

dataset_database_N = WildlifeDataset(metadata_N.df.iloc[100:,:], metadata_N.root, transform=transform)
dataset_query_N = WildlifeDataset(metadata_N.df.iloc[:100,:], metadata_N.root, transform=transform)

query_N, database_N = extractor_N(dataset_query_N), extractor_N(dataset_database_N)

similarity_function = CosineSimilarity()
similarity_N = similarity_function(query_N, database_N)
print(similarity_N)

# Predictions
classifier_N = KnnClassifier(k=1, database_labels=dataset_database_N.labels_string)
predictions_N = classifier_N(similarity_N['cosine'])
print("Predictions for 100 test Images:-\n",predictions_N)
# Accuracy
accuracy_N = np.mean(dataset_query_N.labels_string == predictions_N)
print("Accuracy on NyalaData data: {:.2f}%".format(accuracy_N * 100))
# precision,recall and F1 score
precision_N = precision_score(dataset_query_N.labels_string, predictions_N, average='weighted')
recall_N = recall_score(dataset_query_N.labels_string, predictions_N, average='weighted')
f1_N = f1_score(dataset_query_N.labels_string, predictions_N, average='weighted')
print("Precision:", precision_N)
print("Recall:", recall_N)
print("F1 Score:", f1_N)

"""**StripeSpotter Dataset**"""

# Download dataset (if not already downloaded)
datasets.StripeSpotter.get_data('../data/StripeSpotter')
# Load dataset metadata
metadata = datasets.StripeSpotter('../data/StripeSpotter')
transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])
dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)

dataset_database = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)
dataset_query = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)

query, database = extractor(dataset_query), extractor(dataset_database)

similarity_function = CosineSimilarity()
similarity = similarity_function(query, database)
print(similarity)

classifier = KnnClassifier(k=1, database_labels=dataset_database.labels_string)
predictions = classifier(similarity['cosine'])
print("Predictions for 100 test Images:-\n",predictions)
accuracy = np.mean(dataset_query.labels_string == predictions)
print("Accuracy on StripeSpotter data: {:.2f}%".format(accuracy * 100))
# precision
precision = precision_score(dataset_query.labels_string, predictions, average='weighted')
# recall
recall = recall_score(dataset_query.labels_string, predictions, average='weighted')
# F1 score
f1 = f1_score(dataset_query.labels_string, predictions, average='weighted')

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

"""**IPanda50 Dataset**"""

# Download dataset (if not already downloaded)
datasets.IPanda50.get_data('../data/IPanda50')
# Load dataset metadata
metadata = datasets.IPanda50('../data/IPanda50')
transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])
dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)

dataset_database = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)
dataset_query = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)

query, database = extractor(dataset_query), extractor(dataset_database)

similarity_function = CosineSimilarity()
similarity = similarity_function(query, database)
print(similarity)

classifier = KnnClassifier(k=1, database_labels=dataset_database.labels_string)
predictions = classifier(similarity['cosine'])
print("Predictions for 100 test Images:-\n",predictions)
accuracy = np.mean(dataset_query.labels_string == predictions)
print("Accuracy on IPanda50 data: {:.2f}%".format(accuracy * 100))



"""CZoo Dataset"""

# Download dataset (if not already downloaded)
datasets.CZoo.get_data('../data/CZoo')

# Load dataset metadata
metadata_CZoo = datasets.CZoo('../data/CZoo')
transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])
dataset = WildlifeDataset(metadata_CZoo.df, metadata_CZoo.root, transform=transform)

dataset_database = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)
dataset_query = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)

query, database = extractor(dataset_query), extractor(dataset_database)

similarity_function = CosineSimilarity()
similarity = similarity_function(query, database)
print(similarity)

classifier = KnnClassifier(k=1, database_labels=dataset_database.labels_string)
predictions = classifier(similarity['cosine'])
print("Predictions for 100 test Images:-\n",predictions)
accuracy = np.mean(dataset_query.labels_string == predictions)
print("Accuracy on CZoo data: {:.2f}%".format(accuracy * 100))

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate precision
precision = precision_score(dataset_query.labels_string, predictions, average='weighted')

# Calculate recall
recall = recall_score(dataset_query.labels_string, predictions, average='weighted')

# Calculate F1 score
f1 = f1_score(dataset_query.labels_string, predictions, average='weighted')

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Download dataset (if not already downloaded)
datasets.CowDataset.get_data('../data/CowDataset')

# Load dataset metadata
metadata_CZoo = datasets.CowDataset('../data/CowDataset')
transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])
dataset = WildlifeDataset(metadata_CZoo.df, metadata_CZoo.root, transform=transform)





"""# ***SIFT***"""

pip install timm

pip install wildlife-datasets

pip install wildlife-tools

import timm
import numpy as np
from wildlife_datasets.datasets import MacaqueFaces
from wildlife_tools.data import WildlifeDataset
import torchvision.transforms as T
from wildlife_datasets import datasets, splits
from wildlife_tools.features import DeepFeatures
from wildlife_tools.similarity import CosineSimilarity
from wildlife_tools.inference import KnnClassifier
from sklearn.metrics import precision_score, recall_score, f1_score

from wildlife_tools.features import SIFTFeatures

# Download dataset (if not already downloaded)
datasets.CowDataset.get_data('../data/CowDataset')

# Load dataset metadata
metadata_CZoo = datasets.CowDataset('../data/CowDataset')
transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])
dataset = WildlifeDataset(metadata_CZoo.df, metadata_CZoo.root, transform=transform)





import numpy as np
from PIL import Image
import torchvision.transforms as T
from wildlife_datasets import datasets
from wildlife_tools.data import WildlifeDataset
from wildlife_tools.features import SIFTFeatures
import cv2  # Import OpenCV for SIFT

# Download dataset (if not already downloaded)
datasets.CowDataset.get_data('../data/CowDataset')

# Load dataset metadata
metadata_CZoo = datasets.CowDataset('../data/CowDataset')

# Define transformations: resize, convert to PIL Image, convert to grayscale, and convert to numpy array
transform = T.Compose([
    T.Resize([224, 224]),  # Resize the image
    # T.ToPILImage(),  # Convert tensor to PIL Image
    T.Grayscale(),  # Convert to grayscale
    # T.ToTensor(),  # Convert PIL Image to numpy array
    # lambda x: (x * 255).astype(np.uint8)  # Convert to 8-bit integer format
])

# Create datasets with transformations
dataset = WildlifeDataset(metadata_CZoo.df, metadata_CZoo.root, transform=transform)
dataset_database_CZoo = WildlifeDataset(metadata_CZoo.df.iloc[100:], metadata_CZoo.root, transform=transform)
dataset_query_CZoo = WildlifeDataset(metadata_CZoo.df.iloc[:100], metadata_CZoo.root, transform=transform)

# Initialize SIFT extractor
sift = cv2.SIFT_create()
extractor_CZoo = SIFTFeatures()

query_CZoo, database_CZoo = extractor_CZoo(dataset_query_CZoo), extractor_CZoo(dataset_database_CZoo)

print(f'First 5 query features shape: {[i.shape for i in query_CZoo[:5]]}')
print(f'First 5 database features shape: {[i.shape for i in database_CZoo[:5]]}')

import timm
import pandas as pd
import torchvision.transforms as T

from wildlife_tools.data import WildlifeDataset, SplitMetadata
from wildlife_tools.features import SIFTFeatures
from wildlife_tools.similarity import MatchDescriptors
from wildlife_tools.inference import KnnClassifier

similarity = MatchDescriptors(descriptor_dim=128, thresholds=[0.8])
sim = similarity(query_CZoo, database_CZoo)[0.8]

print("Number of SIFT correspondences after 0.8 ratio test threshold: \n", sim)

classifier_CZoo = KnnClassifier(k=1, database_labels=dataset_database_CZoo.labels_string)
predictions_CZoo = classifier_CZoo(sim)

print("Predictions for 100 test Images:-\n",predictions_CZoo)
accuracy_CZoo = np.mean(dataset_query_CZoo.labels_string == predictions_CZoo)
print("Accuracy on MacaqueFaces data: {:.2f}%".format(accuracy_CZoo * 100))

# Nearest neigbour classifier using the similarity
classifier = KnnClassifier(k=1, database_labels=dataset_database_CZoo.labels_string)
preds = classifier(sim)
print("Prediction \t", preds)
print("Ground truth \t", dataset_query_CZoo.labels_string)

acc = sum(preds == dataset_query_CZoo.labels_string) / len(dataset_query_CZoo.labels_string)
print('\n Accuracy: ', acc*100,"%")